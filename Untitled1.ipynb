{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "parliamentary-flexibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoTokenizer\n",
    "\n",
    "# load pretrained model\n",
    "\n",
    "tokenizer = Wav2Vec2Processor.from_pretrained(\n",
    "    \"/root/develop/wav2vec2-infer/processor_save\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"/root/develop/wav2vec2-infer/checkpoints/checkpoint-5400\")\n",
    "\n",
    "model2 = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\")\n",
    "tokenizer2 = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "model3 = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"/root/develop/wav2vec2-infer/checkpoints/checkpoint-300\")\n",
    "\n",
    "model4 = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"/root/develop/wav2vec2-infer/checkpoints/checkpoint-600\")\n",
    "\n",
    "# load audio\n",
    "audio_input, _ = sf.read(\"ko.wav\")\n",
    "audio_input2, _ = sf.read(\"en.flac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "copyrighted-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe\n",
    "input_values = tokenizer(audio_input, return_tensors=\"pt\",\n",
    "                         sampling_rate=16000).input_values\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSCRIPTION\n",
      "\n",
      "torch.Size([550])\n"
     ]
    }
   ],
   "source": [
    "print(\"TRANSCRIPTION\")\n",
    "print(transcription)\n",
    "print(predicted_ids[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developed-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271],\n",
       "         [  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271],\n",
       "         [  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271],\n",
       "         ...,\n",
       "         [  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271],\n",
       "         [  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271],\n",
       "         [  5.7200, -12.9688, -12.4418,  ..., -12.6546,  -5.4650,  -3.5271]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accessory-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = tokenizer(audio_input, return_tensors=\"pt\",\n",
    "                         sampling_rate=16000).input_values\n",
    "logits = model2(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer2.batch_decode(predicted_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "traditional-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEMIONG AN PEPOINTIN THE OCOPILLI DE BULLION GIRIBURIS COUGET TO NEGUAN AND GO ME ON ONCE COTTY THERE ALSO ONDRIOS O BUUSHOTETOUTRRANAND GET YONG GEJOGINI CAR'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "realistic-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = tokenizer(audio_input, return_tensors=\"pt\",\n",
    "                         sampling_rate=16000).input_values\n",
    "logits = model3(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "removed-commitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.2375, -3.1034, -3.4086,  ..., -3.4551, -3.2272, -1.7203],\n",
       "         [ 8.2644, -3.2179, -3.3648,  ..., -3.3933, -3.3824, -1.6037],\n",
       "         [ 8.9430, -2.8540, -3.2314,  ..., -3.4115, -3.2148, -1.6347],\n",
       "         ...,\n",
       "         [ 9.2414, -2.6546, -3.0847,  ..., -3.1815, -3.0496, -1.4093],\n",
       "         [ 9.3501, -2.6271, -3.0545,  ..., -3.2305, -3.0129, -1.4624],\n",
       "         [ 9.3492, -2.6078, -3.0972,  ..., -3.1654, -3.0332, -1.4182]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stone-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.6147, -3.7512, -3.7859,  ..., -3.8223, -3.0981, -1.9036],\n",
       "         [10.6441, -3.7430, -3.7736,  ..., -3.8124, -3.0947, -1.8948],\n",
       "         [10.6243, -3.7467, -3.7840,  ..., -3.8260, -3.0994, -1.9039],\n",
       "         ...,\n",
       "         [ 8.6110, -4.3183, -4.3866,  ..., -4.3545, -3.0555, -1.6499],\n",
       "         [ 9.3459, -4.0873, -4.1605,  ..., -4.1853, -2.9888, -1.6603],\n",
       "         [ 9.3993, -4.0709, -4.1521,  ..., -4.1736, -2.9816, -1.6580]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values = tokenizer(audio_input, return_tensors=\"pt\",\n",
    "                         sampling_rate=16000).input_values\n",
    "logits = model4(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stylish-incident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "living-kingston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hahahaha'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from korean_romanizer.romanizer import Romanizer\n",
    "\n",
    "r = Romanizer(\"하하하하\")\n",
    "r.romanize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-virtue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
